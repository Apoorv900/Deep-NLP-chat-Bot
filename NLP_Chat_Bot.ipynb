{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjDr4O6IcP7m"
   },
   "source": [
    "# Building a Chat Bot with Deep NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEN40a9qcP7z"
   },
   "source": [
    "The project has been split into 4 parts: \n",
    "\n",
    "Part 1 : Data Preprocessing\n",
    "\n",
    "Part 2 : Building the Seq2Seq model\n",
    "\n",
    "Part 3 : Training the Seq2Seq model\n",
    "\n",
    "Part 4 :Testing the Seq2Seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhjyj-u3cP71"
   },
   "source": [
    "# Importing the libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbLF1Nl0cP73"
   },
   "source": [
    "1. numpy library to work with arrays\n",
    "\n",
    "2. tensorflow for deep learning\n",
    "\n",
    "3. regular expression library to clean the text \n",
    "\n",
    "4. time library to measure the training time of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xcxUNSTcP74"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60ax7O1icP78"
   },
   "source": [
    "# PART 1 - DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzuSHYDRcP79"
   },
   "source": [
    "# Importing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sa2P1tvUcP7-"
   },
   "source": [
    "We are going to give two variable names for the data sets.\n",
    "\n",
    "The dataset of \"lines\" as lines and\n",
    "\n",
    "the dataset of \"conversations\" as conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cKkxzBicP8A"
   },
   "outputs": [],
   "source": [
    "lines = open('movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlqNO47DcP8B"
   },
   "source": [
    "To avoid encoding issue : pass argument encoding = 'utf-8'\n",
    "\n",
    "To ignore the insignifacant errors , we can do : errors = 'ignore'\n",
    "\n",
    ".split('\\n') is done to split the lines of the dataset with respect to occurence of a new line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7itdaCpcP8C"
   },
   "outputs": [],
   "source": [
    "conversations = open('movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRxa2gYccP8E"
   },
   "source": [
    "Let's have a look at the lines dataset\n",
    "\n",
    "\n",
    "'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n",
    " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
    " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n",
    " 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n",
    " \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\",\n",
    " 'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow',\n",
    " \"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\",\n",
    " 'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation\n",
    "\n",
    "# \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\",\n",
    "#  \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n",
    "#  \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n",
    "#  \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n",
    "#  \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\","
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xP0tbBQqcP8J"
   },
   "source": [
    "# Creating a dictionary that maps each line and its id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jw-zbJGOcP8L"
   },
   "source": [
    "We want to create a dataset composing of input and outputs, and the easiest way to do that is by maintaining a dictionary.\n",
    "\n",
    "We want to map each line with it's ID.\n",
    "\n",
    "So the key identifier of the dictionary will be the ID of the line and the value will be the line itself.\n",
    "\n",
    "1. declare an empty dictionary\n",
    "\n",
    "2. iterate through all the lines of the \"lines\" dataset\n",
    "    a)now for each of the lines, split the line with respect to \" +++$+++ \"\n",
    "    \n",
    "    b)now get the first element and place it as the key, and get the last value       and place it as the value\n",
    "    \n",
    "    c) we are just placing an if statement to ensure that the line has 5              elements for splitting, else we might face some shfiting issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sauFyQYAcP8M"
   },
   "outputs": [],
   "source": [
    "id2line = {}\n",
    "#iterate through all lines in the lines dataset\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id2line \n",
    "\n",
    "# 'L169795': 'And?',\n",
    "#  'L261935': 'Dad!',\n",
    "#  'L341879': 'What do you see, Starling?',\n",
    "#  'L306525': 'Please -- was at the time brandishing your firearm, trying in his rage to shoot an acquaintance -- friend of long standing --',\n",
    "#  'L612204': 'AH UGH.',\n",
    "#  'L73334': 'Bruce Wayne. In the flesh.',"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftkR5toccP8O"
   },
   "source": [
    "# Creating a list of all conversations with the IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHcceCgAcP8P"
   },
   "outputs": [],
   "source": [
    "conversations_ids = []\n",
    "for conversation in conversations[:-1]:\n",
    "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
    "    conversations_ids.append(_conversation.split(','))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation_ids list\n",
    "# ['L194', 'L195', 'L196', 'L197'],\n",
    "#  ['L198', 'L199'],\n",
    "#  ['L200', 'L201', 'L202', 'L203'],\n",
    "#  ['L204', 'L205', 'L206'],\n",
    "#  ['L207', 'L208']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKQPAnAIcP8S"
   },
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "for conversation in conversations_ids:\n",
    "    for i in range(len(conversation) - 1):\n",
    "        questions.append(id2line[conversation[i]])\n",
    "        answers.append(id2line[conversation[i+1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1FThoa-cP8W"
   },
   "source": [
    "Let's have a look at the answers list we created above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyTUqsJfcP8X"
   },
   "source": [
    "# Cleaning of the texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2g84qXHScP8Y"
   },
   "source": [
    "1. put everything in lowercase\n",
    "\n",
    "2. remove all the apostrophes\n",
    "\n",
    "3. removing all non-essential words\n",
    "\n",
    "creating a function that will carry out all of the above activities\n",
    "\n",
    "Steps 1 and 2 will be carried out in the below function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAXtCW3ycP8Y"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "na6cMrDGcP8Z"
   },
   "source": [
    "# Applying the above function for cleaning the questions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00KCj1OFcP8a"
   },
   "outputs": [],
   "source": [
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCvNLkT-cP8b"
   },
   "source": [
    "# Applying the clean_text function for cleaning the questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkTmTrBGcP8b"
   },
   "outputs": [],
   "source": [
    "clean_answers = []\n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtG_km7dcP8c"
   },
   "source": [
    "Let's have a look at the clean_questions list we created above \n",
    "\n",
    "\n",
    "'can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again',\n",
    " 'well i thought we would start with pronunciation if that is okay with you',\n",
    " 'not the hacking and gagging and spitting part  please',\n",
    " 'you are asking me out  that is so cute what is your name again',\n",
    " \"no no it's my fault  we didn't have a proper introduction \",\n",
    " 'cameron',"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mGDx3OQcP8d"
   },
   "source": [
    "Let's have a look at the clean_answers list we created above\n",
    "\n",
    "'well i thought we would start with pronunciation if that is okay with you',\n",
    " 'not the hacking and gagging and spitting part  please',\n",
    " \"okay then how 'bout we try out some french cuisine  saturday  night\",\n",
    " 'forget it',\n",
    " 'cameron',\n",
    " 'the thing is cameron  i am at the mercy of a particularly hideous breed of loser  my sister  i cannot date until she does',\n",
    " 'seems like she could get a date easy enough',\n",
    " 'unsolved mystery  she used to be really popular when she started high school then it was just like she got sick of it or something',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gl9oh2encP8e"
   },
   "outputs": [],
   "source": [
    "word2count = {}\n",
    "#going through the clean_questions\n",
    "for question in clean_questions:\n",
    "    for word in question.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "#going through the clean_answers\n",
    "for answer in clean_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2o2IfTpcP8f"
   },
   "source": [
    "# Tokenization and filtering of the non-frequent words\n",
    "\n",
    "1. We will assign a unique integer to each of the words present in the clean_questions and clean_answers list \n",
    "\n",
    "2. for each of the words, we will compare the number of occurences (with the help of the word2count{} dictionary we created above) of that particular word and filter out those words which do not cross the threshold,i.e, removal of infrequent words\n",
    "\n",
    "3. and for those words which cross the threshold, we will assign a unique integer value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ryNFoeEcP8f"
   },
   "outputs": [],
   "source": [
    "threshold_questions = 20\n",
    "threshold_answers = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfSjWXtfcP8g"
   },
   "source": [
    "The threshold is a hyperparameter in NLP, and please feel free to experiment with this value while training your model. But make sure not to keep the threshold value too low, else it might be too overwhelming for the model to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2BtNXAtcP8g"
   },
   "outputs": [],
   "source": [
    "# carrying out the above mentioned steps for questions\n",
    "questionswords2int = {}\n",
    "word_number = 0\n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold_questions:\n",
    "        questionswords2int[word] = word_number\n",
    "        word_number += 1\n",
    "        \n",
    "# carrying out the above mentioned steps for answers\n",
    "answerswords2int = {}\n",
    "word_number = 0\n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold_answers:\n",
    "        answerswords2int[word] = word_number\n",
    "        word_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSY_k9qCcP8h"
   },
   "source": [
    "Let's have a look at the questionswords2int dictionary we created above\n",
    " \n",
    " 'dowd': 2252,\n",
    " 'december': 7082,\n",
    " 'zuzu': 3282,\n",
    " 'impressed': 4407,\n",
    " 'ditch': 7778,\n",
    " 'bark': 2253,\n",
    " 'slut': 0,\n",
    " 'scattered': 8479,\n",
    " 'pa': 4033,\n",
    " 'italian': 1,\n",
    " 'box': 2056,\n",
    " 'thirteen': 8423,\n",
    " 'gone!': 7780,\n",
    " 'legit': 4409,\n",
    " 'health': 3334,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTGXddi4cP8j"
   },
   "source": [
    "Let's have a look at the answerswords2int dictionary we created above\n",
    "\n",
    " 'dowd': 2252,\n",
    " 'december': 7082,\n",
    " 'zuzu': 3282,\n",
    " 'impressed': 4407,\n",
    " 'ditch': 7778,\n",
    " 'bark': 2253,\n",
    " 'slut': 0,\n",
    " 'scattered': 8479,\n",
    " 'pa': 4033,\n",
    " 'italian': 1,\n",
    " 'box': 2056,\n",
    " 'thirteen': 8423,\n",
    " 'gone!': 7780,\n",
    " 'legit': 4409,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqIG_MsrcP8k"
   },
   "source": [
    "# Adding the last tokens to the above two dictionaries\n",
    "\n",
    "EOS = End of sentence\n",
    "\n",
    "SOS = Start of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSTcD_cScP8m"
   },
   "outputs": [],
   "source": [
    "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8PMgnz5cP8o"
   },
   "outputs": [],
   "source": [
    "for token in tokens:\n",
    "    questionswords2int[token] = len(questionswords2int) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7fI6fdWcP8p"
   },
   "source": [
    "Doing the same as above to answerswords2int dictionary\n",
    "\n",
    "Here we need to assign a unique integer to token in the answerswords2int dictionary. Since words are added, by incrementing one at a time, hence here we just calculate the length of the dictionary and 1 to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBeiI4V_cP8p"
   },
   "outputs": [],
   "source": [
    "for token in tokens:\n",
    "    answerswords2int[token] = len(answerswords2int) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5ybv2BecP8q"
   },
   "source": [
    "# Creating an inverse dictionary of the answerswords2int dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6txqg_UcP8q"
   },
   "source": [
    "We are doing so because we need the inverse mapping from integers to answers words in the implementation of the Seq2Seq architectural model. \n",
    "Also, we need this only for the answerswords dictionary and not for questionswords.\n",
    "\n",
    "1. declare a new dictionary in python\n",
    "\n",
    "2. there is a single line trick in python to reverse the mapping of a dictionary,i.e, to interchange the key-value pairs and store them in a new dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7NFQzn1cP8q"
   },
   "outputs": [],
   "source": [
    "answersints2word = {w_i: w for w, w_i in answerswords2int.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omch1x6zcP8r"
   },
   "source": [
    "Let's have a look at the newly created answersints2word dictionary (inverse mapped dictionary)\n",
    " 0: 'slut',\n",
    " 1: 'italian',\n",
    " 2: 'bend',\n",
    " 3: 'power',\n",
    " 4: 'skin',\n",
    " 5: 'labor',\n",
    " 6: 'bore',\n",
    " 7: 'spaghetti',\n",
    " 8: 'king!',\n",
    " 9: 'sneaking',\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBxr_CWmcP8s"
   },
   "source": [
    "# Adding the End Of String token to the end of every answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DuO6e8ZcP8s"
   },
   "outputs": [],
   "source": [
    "for i in range(len(clean_answers)):\n",
    "    clean_answers[i] += ' <EOS>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHBcT_WScP8t"
   },
   "source": [
    "Let's have a look at the modified clean_answers list after appending the EOS token\n",
    "\n",
    "'well i thought we would start with pronunciation if that is okay with you <EOS>',\n",
    " 'not the hacking and gagging and spitting part  please <EOS>',\n",
    " \"okay then how 'bout we try out some french cuisine  saturday  night <EOS>\",\n",
    " 'forget it <EOS>',\n",
    " 'cameron <EOS>',\n",
    " 'the thing is cameron  i am at the mercy of a particularly hideous breed of loser  my sister  i cannot date until she does <EOS>',\n",
    " 'seems like she could get a date easy enough <EOS>',\n",
    " 'unsolved mystery  she used to be really popular when she started high school then it was just like she got sick of it or something <EOS>',"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqFjIRjacP8t"
   },
   "source": [
    "# Tranalating all the questions and the answers into integers and replacing all the words that were filtered out by \"< OUT>\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mmHbOBjcP8u"
   },
   "outputs": [],
   "source": [
    "questions_into_int = []\n",
    "for question in clean_questions:\n",
    "    ints = [] # list of integers, which will be the associated integer to each of the word present in that question\n",
    "    for word in question.split():\n",
    "        if word not in questionswords2int:\n",
    "            ints.append(questionswords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(questionswords2int[word])\n",
    "    questions_into_int.append(ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS2MnK1EcP8u"
   },
   "source": [
    "Doing the same set of above steps for answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8kOAoqecP8v"
   },
   "outputs": [],
   "source": [
    "answers_into_int = []\n",
    "for answer in clean_answers:\n",
    "    ints = [] # list of integers, which will be the associated integer to each of the word present in that question\n",
    "    for word in answer.split():\n",
    "        if word not in answerswords2int:\n",
    "            ints.append(answerswords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(answerswords2int[word])\n",
    "    answers_into_int.append(ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiKJ5yUScP8v"
   },
   "source": [
    "Let's have a look at the questions_to_int \n",
    "\n",
    "[8065,\n",
    "  3783,\n",
    "  3552,\n",
    "  6773,\n",
    "  1310,\n",
    "  8824,\n",
    "  8824,\n",
    "  6112,\n",
    "  3217,\n",
    "  8824,\n",
    "  2293,\n",
    "  72,\n",
    "  4780,\n",
    "  3347,\n",
    "  8824,\n",
    "  4244,\n",
    "  5451,\n",
    "  4866,\n",
    "  2513,\n",
    "  6233,\n",
    "  8824,\n",
    "  5107],\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqMI8GzVcP8w"
   },
   "source": [
    "Let's have a look at the answers_into_int list\n",
    "[3164,\n",
    "  2079,\n",
    "  4359,\n",
    "  3783,\n",
    "  134,\n",
    "  3579,\n",
    "  1036,\n",
    "  8824,\n",
    "  6166,\n",
    "  8419,\n",
    "  435,\n",
    "  649,\n",
    "  1036,\n",
    "  4069,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMFyTFF2cP8x"
   },
   "source": [
    "# Sorting questions and answers by the length of the questions\n",
    "\n",
    "We are doing so because it will speed up the training and help to reduce the loss. The reason for this is because it will reduce the amount of padding during training.\n",
    "\n",
    "1.create two empty lists in python\n",
    " one called sorted_clean_answers[] and the other sorted_clean_questions[]\n",
    " \n",
    " We will place a limit on the length of the questions, because very lengthy questions will be too overwhelming for the chatbot to learn from. \n",
    "Also this limit on the length can be considered as a hyperparamater which can be tuned to get better perfomance.(here let's take the limit to be 25)\n",
    "\n",
    "2. looping over different possible lengths of the questions(upto the limit)\n",
    "    for each of the questions, we need to get two important elements - index of     the question and the question itself\n",
    "    \n",
    "    the trick to get these two elements at the same time is to use the enumerate function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64PASdpkcP8y"
   },
   "outputs": [],
   "source": [
    "sorted_clean_questions = []\n",
    "sorted_clean_answers = []\n",
    "for length in range(1, 25 + 1):\n",
    "    for i in enumerate(questions_into_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_clean_questions.append(questions_into_int[i[0]])\n",
    "            sorted_clean_answers.append(answers_into_int[i[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qas6KEsRcP80"
   },
   "source": [
    "-------------------------------------------------------------------------------\n",
    "# PART 2 - BUILDING THE SEQ2SEQ MODEL\n",
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHSPDOqacP81"
   },
   "source": [
    "# Creating placeholders for the inputs and the targets\n",
    "\n",
    "In TensorFlow, all avariables are used in tensors. Tensors are like an advanced numpy array that allows very fast computations in Deep Neural Networks.\n",
    "\n",
    "All variables used as tensors must be defined as what we call TensorFlow placeholders.\n",
    "\n",
    "This is more of an advanced data structure that can contain tensors and also additional features.\n",
    "\n",
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOEUr6OhcP82"
   },
   "source": [
    "We will be defining a function that will call model inputs, and inside this function we will create a placeholder for the inputs and a placeholder for the targets.\n",
    "Then we will add a learning rate and even more hybrid parameters.\n",
    "\n",
    "In short, we will be creating placeholders to be able to use these variables in future training.\n",
    "\n",
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsqOolFMcP83"
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None,None], name = 'input')\n",
    "    targets = tf.placeholder(tf.int32, [None,None], name = 'target')\n",
    "    lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32,name = 'keep_prob')\n",
    "    return inputs,targets,lr,keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sA6t1bZGcP83"
   },
   "source": [
    "# Preprocessing the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gr7QtM0lcP84"
   },
   "outputs": [],
   "source": [
    "def preprocess_targets(targets, word2int, batch_size):\n",
    "    left_side = tf.fill([batch_size, 1], word2int['<SOS>'])\n",
    "    right_side = tf.strided_slice(targets, [0,0], [batch_size,-1], [1,1])\n",
    "    preprocessed_targets = tf.concat([left_side, right_side], 1)\n",
    "    return preprocessed_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPD6tYcDcP85"
   },
   "source": [
    "-------------------------------------------------------------------------------\n",
    "\n",
    "# Creating the Encoder RNN Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDxdZhQ-cP85"
   },
   "source": [
    "The arguments of this function include: \n",
    "\n",
    "1. rnn_inputs that corresponds to the model inputs\n",
    "\n",
    "2. rnn_size is the number of input tensors of the encoder\n",
    "\n",
    "3. num_layers, the number of layers in the RNN\n",
    "\n",
    "4. keep_prob, for dropout regularization(to improve accuracy)\n",
    "\n",
    "5. sequence_length, which is the list of th elength of each question in the batch.\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "1. In tensorflow we have an amazing class that will help us create an LSTM\n",
    "\n",
    "assign variable lstm  to tf.contrib(module).rnn(submodule).BasicLSTMCell(rnn_size) \n",
    "\n",
    "2. assign variable lstm_dropout to  tf.contrib(module).rnn(submodule).DropoutWrapper(class)(lstm, keep_prob)\n",
    "\n",
    "3. we are now ready to create the encoder cell\n",
    "\n",
    "assign variable encoder_cell to tf.contrib(module).rnn(submodule).MultiRNNCell(the number of lstm dropout mulitplied to number of layers we have as an argument to the function)\n",
    "\n",
    "4. now to get the encoder_state, we will get ot from the bidirectional_dynamic_rnn function from the nn module by tensorflow\n",
    "\n",
    "the above step created a dynamic version of a bidirectional RNN(this will help us in making our chatbot more powerful)\n",
    "\n",
    "the dynamic version of bidirectional rnn will take the input and build independent forward and abckward RNN's. \n",
    "\n",
    "NOTE: We need to make sure in case of dynamic bidirectional RNNs that inpput size of forward cell and backward cell must match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZbTVk5AcP86"
   },
   "outputs": [],
   "source": [
    "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "    _, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell, \n",
    "                                                       cell_bw = encoder_cell, \n",
    "                                                       sequence_length = sequence_length, \n",
    "                                                       inputs = rnn_inputs,\n",
    "                                                       dtype = tf.float32)\n",
    "    return encoder_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ritMika3cP86"
   },
   "source": [
    "-------------------------------------------------------------------------------\n",
    "\n",
    "# Creating the Decoder of the RNN layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xK8gjgcwcP9D"
   },
   "source": [
    "# Step 1. Decoding the training set\n",
    "\n",
    "The arguments of this function include:\n",
    "\n",
    "1. encoder_state\n",
    "\n",
    "2. decoder_cell\n",
    "\n",
    "3. decoder_embedded_input\n",
    "\n",
    "4. sequence_length\n",
    "\n",
    "5. decoding_scope\n",
    "\n",
    "6. output_function\n",
    "\n",
    "7. keep_prob\n",
    "\n",
    "8. batch_size\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "1. The first thing we need to do is get the attention states\n",
    "\n",
    "initialize attention_states as a 3 dimesnional matrix initialized with zeros\n",
    "\n",
    "Since we are dealing with batches, the number of lines is going to be batch_size\n",
    "\n",
    "The number of elements on the third axis is going to be decoder_cell.output_size\n",
    "\n",
    "2. We will get the attention_keys, the attention_values, the atention_score_function and attention_construct_function using the TensorFlow function belonging to the seq2seq submodule which is prepare_attention()\n",
    "\n",
    "the attention_keys is the keys that are to be compared with the target_states\n",
    "\n",
    "the attention_values, the values that we will use to construct the context vectors\n",
    "\n",
    "the attention_score_function is used to compute the similarity in between keys and the target \n",
    "\n",
    "the attention_construct_function is function used to build the attention state\n",
    "\n",
    "3. The next step is to get the training_decoder_function that will do the decoding of the training set. \n",
    "\n",
    "training_decoder_function is obtained from another tensorflow function present in the seq2seq submodule called attention_decoder_fn_train()\n",
    "\n",
    "\n",
    "4. the next step is to get the decoder_output, decoder_final_state and the decoder_final_context_state(but we need only the decoder_output)\n",
    "\n",
    "this is obtained from the function present in the dynamic_rnn_encoder submodule present in the TensorFlow library\n",
    "\n",
    "5. the final step is to apply dropout to our decoder_output\n",
    "decoder_output_dropout = tf.nn(module).dropout()\n",
    "\n",
    "6. return the output_function(decoder_output_dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AFBV6SlcP9E"
   },
   "outputs": [],
   "source": [
    "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              name = \"attn_dec_train\")\n",
    "    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                              training_decoder_function,\n",
    "                                                                                                              decoder_embedded_input,\n",
    "                                                                                                              sequence_length,\n",
    "                                                                                                              scope = decoding_scope)\n",
    "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
    "    return output_function(decoder_output_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcCHj8V2cP9E"
   },
   "source": [
    "# Step 2. Decoding the test/validation set\n",
    "\n",
    "\n",
    "Here are we are going to make a very similar function as above, but for the observatoins of the test set and the validation set.\n",
    "\n",
    "These are new observations that will not be used in the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pPNsgIOocP9F"
   },
   "outputs": [],
   "source": [
    "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
    "                                                                              encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              decoder_embeddings_matrix,\n",
    "                                                                              sos_id,\n",
    "                                                                              eos_id,\n",
    "                                                                              maximum_length,\n",
    "                                                                              num_words,\n",
    "                                                                              name = \"attn_dec_inf\")\n",
    "    test_predictions, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                                test_decoder_function,\n",
    "                                                                                                                scope = decoding_scope)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uvA8ngHcP9G"
   },
   "source": [
    "# Step 3. Creating the Decoder RNN\n",
    "\n",
    "This function will have the following arguments:\n",
    "\n",
    "1.decoder_rnn\n",
    "\n",
    "2.decoder_embeddings_matrix\n",
    "\n",
    "3.encoder_state  // output of the encoder becomes input of the decoder\n",
    "\n",
    "4.num_words //total number of words in our corpus of words\n",
    "\n",
    "5.sequence_length \n",
    "\n",
    "6.rnn_size // number of layers we want in our RNN decoder\n",
    "\n",
    "7.num_layers\n",
    "\n",
    "8.word2int //the dictionary which we have defined earlier\n",
    "\n",
    "9.keep_prob // for the dropout(regularization) rate\n",
    "\n",
    "10.batch_size \n",
    "\n",
    "-----------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DTrs6igBcP9H"
   },
   "outputs": [],
   "source": [
    "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size):\n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "        weights = tf.truncated_normal_initializer(stddev = 0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_function = lambda x: tf.contrib.layers.fully_connected(x,\n",
    "                                                                      num_words,\n",
    "                                                                      None,\n",
    "                                                                      scope = decoding_scope,\n",
    "                                                                      weights_initializer = weights,\n",
    "                                                                      biases_initializer = biases)\n",
    "        training_predictions = decode_training_set(encoder_state,\n",
    "                                                   decoder_cell,\n",
    "                                                   decoder_embedded_input,\n",
    "                                                   sequence_length,\n",
    "                                                   decoding_scope,\n",
    "                                                   output_function,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size)\n",
    "        decoding_scope.reuse_variables()\n",
    "        test_predictions = decode_test_set(encoder_state,\n",
    "                                           decoder_cell,\n",
    "                                           decoder_embeddings_matrix,\n",
    "                                           word2int['<SOS>'],\n",
    "                                           word2int['<EOS>'],\n",
    "                                           sequence_length - 1,\n",
    "                                           num_words,\n",
    "                                           decoding_scope,\n",
    "                                           output_function,\n",
    "                                           keep_prob,\n",
    "                                           batch_size)\n",
    "    return training_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1u1Vxe6lcP9H"
   },
   "source": [
    "-----------------------------------------------------------------------------\n",
    "\n",
    "# Building the Seq2Seq Model\n",
    "\n",
    "This function is the final ultimate function which we will build using the above defined functions. This will be the brain of our chatbot. \n",
    "\n",
    "This function will take the following arguments:\n",
    "\n",
    "1.inputs which are the questions of the Cornell movie corpus dialogue dataset\n",
    "\n",
    "2.targets. which will be the answers to our questions\n",
    "\n",
    "3.keep_prob\n",
    "\n",
    "4.batch_size\n",
    "\n",
    "5.sequence_length\n",
    "\n",
    "6.answers_num_words\n",
    "\n",
    "7.questions_num_words\n",
    "\n",
    "8.encoder_embedding_size, which is the number of dimensions of the embedding matrix for the encoder\n",
    "\n",
    "9.decoder_embedding_size, which is the number of dimesnions of the embeddig matrix for the decoder\n",
    "\n",
    "10.rnn_size\n",
    "\n",
    "11.num_layers\n",
    "\n",
    "12.questionswords2int, dictionary which we defined previously to preprocess the targets\n",
    "\n",
    "-----------------------------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8ckf0dtcP9I"
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionswords2int):\n",
    "    encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs,\n",
    "                                                              answers_num_words + 1,\n",
    "                                                              encoder_embedding_size,\n",
    "                                                              initializer = tf.random_uniform_initializer(0, 1))\n",
    "    encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "    preprocessed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n",
    "    decoder_embeddings_matrix = tf.Variable(tf.random_uniform([questions_num_words + 1, decoder_embedding_size], 0, 1))\n",
    "    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n",
    "    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input,\n",
    "                                                         decoder_embeddings_matrix,\n",
    "                                                         encoder_state,\n",
    "                                                         questions_num_words,\n",
    "                                                         sequence_length,\n",
    "                                                         rnn_size,\n",
    "                                                         num_layers,\n",
    "                                                         questionswords2int,\n",
    "                                                         keep_prob,\n",
    "                                                         batch_size)\n",
    "    return training_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GU9omZemcP9I"
   },
   "source": [
    "-------------------------------------------------------------------------------\n",
    "\n",
    "# PART 3 -  TRAINING THE SEQ2SEQ MODEL\n",
    "\n",
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijPsWatPcP9J"
   },
   "source": [
    "# Setting the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8o5Y_W58cP9J"
   },
   "source": [
    "The whole process of getting the batches of input into the neural network and then forward propagating them inside the encoders in the encoder states and then forward propagating the encoder states with targets inside the deep recurrent neural network to get the final answers/outputs.Then back-propagating the loss generated by the outputs and the targets back into the neural network and updating the weights towards the direction of a better ability for the chatbot to speak like a human. \n",
    "\n",
    "1.An epoch is basically one whole iteration of the above mentioned steps(take 100, if training is taking too long, adjust it to 50, but not lower).\n",
    "\n",
    "2.batch_size, we are setting it to be 64(usually a power of 2)\n",
    "\n",
    "3.rnn_size, we are setting it to be 512\n",
    "\n",
    "4.num_layers, we are setting it to be 3, can change later if necessary\n",
    "\n",
    "5.encoding_embedding_size,(number of columns in the embedded matrix), taken as 512\n",
    "\n",
    "6.similarly, decoding_embedding_size is taken to be as 512\n",
    "\n",
    "7.learning_rate, we will start with 0.01\n",
    "\n",
    "8.learning_rate_decay, which represents the percentage by wich learning_rate is reduced over the iterations of the training\n",
    "\n",
    "9.min_learning_rate, as a lower bound to take care of early stopping if the learning_rate decreases drastically\n",
    "\n",
    "10.keep_probability, dropout regularization hyperparameter to prevent overfitting\n",
    "According to Geoffery Hinton, the master of Deep Learning and Artificial Intelligence, in his paper states: dropping out 20% of the input units and 50% of the hidden units was often found to be optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e95GwptqcP9K"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "rnn_size = 512\n",
    "\n",
    "num_layers = 3\n",
    "\n",
    "encoding_embedding_size = 512\n",
    "\n",
    "decoding_embedding_size = 512\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "learning_rate_decay = 0.9\n",
    "\n",
    "min_learning_rate = 0.0001\n",
    "\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSlce6WdcP9K"
   },
   "source": [
    "-------------------------------------------------------------------------------\n",
    "\n",
    "# Defining a session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKkQCJpLcP9L"
   },
   "source": [
    "We will define a TensorFlow session in which all the tensorflow training will be run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2MSv5aqcP9L"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3FUZBKecP9M"
   },
   "source": [
    "-------------------------------------------------------------------------------\n",
    "\n",
    "# Loading the Model Inputs\n",
    "\n",
    "We will be using a function which we defined previously in Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzBUUE6_cP9M"
   },
   "outputs": [],
   "source": [
    "inputs, targets, lr, keep_prob = model_inputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXCN7-cfcP9M"
   },
   "source": [
    "-------------------------------------------------------------------------------\n",
    "\n",
    "# Setting the sequence length\n",
    "\n",
    "We are going to set the sequence length to maximum length which will be 25(which we have already done in end of part 1 - data preprocessing).\n",
    "\n",
    "\n",
    "We are going to use tensorflow palceholder with default function.\n",
    "\n",
    "1.The arguments will be: maximum_length(25)\n",
    "\n",
    "2.Sequence shape, since there is no tensor to deal with, input None\n",
    "\n",
    "3.name of the sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evBOXJU3cP9N"
   },
   "outputs": [],
   "source": [
    "sequence_length = tf.placeholder_with_default(25, None, name = 'sequence_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Z0yc1vZcP9N"
   },
   "source": [
    "-------------------------------------------------------------------------------\n",
    "\n",
    "# Getting the shape of the input tensor\n",
    "\n",
    "We need to get the shape of the input because this will be one of the arguments of one specific function we will use for training.\n",
    "\n",
    "The specific function is actually the ones function by tensorflow(created a tensor of ones)\n",
    "\n",
    "For this we will use shape function of tenorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lp_3NHM0cP9O"
   },
   "outputs": [],
   "source": [
    "input_shape = tf.shape(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ur2z8r63cP9O"
   },
   "source": [
    "-------------------------------------------------------------------------------\n",
    "\n",
    "# Getting the training and test predictions\n",
    "\n",
    "We will use the function seq2seq_model we defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e54dBo9ZcP9O"
   },
   "outputs": [],
   "source": [
    "training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]),\n",
    "                                                       targets,\n",
    "                                                       keep_prob,\n",
    "                                                       batch_size,\n",
    "                                                       sequence_length,\n",
    "                                                       len(answerswords2int),\n",
    "                                                       len(questionswords2int),\n",
    "                                                       encoding_embedding_size,\n",
    "                                                       decoding_embedding_size,\n",
    "                                                       rnn_size,\n",
    "                                                       num_layers,\n",
    "                                                       questionswords2int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFc8XTv5cP9P"
   },
   "source": [
    "--------------------------------------------------------------------------------\n",
    "# Setting up the loss error, the Optimizer and Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85ReVk7dcP9P"
   },
   "source": [
    "We are going to define a new scope here which will contain two final elements that we will use for the training:\n",
    "\n",
    "1.The loss error - weighted cross entropy loss error\n",
    "\n",
    "2.Optimizer with gradient clipping -Adam Optimizer and then apply gradient clipping to avoid exploding and vanishing gradient issues\n",
    "\n",
    "We will obtain our loss_error from the sequence loss function present in the seq2seq submodule present in the contrib module.\n",
    "It takes the arguments the training_predictions and the targets.\n",
    "This is necessary as we will be calculating our loss_error based on the difference between the two.\n",
    "The third argument is the tensor of weights initialized to ones to appropriate shape.\n",
    "\n",
    "We will get the optimizer which will be an object of the AdamOptimizer class which is a class in tensorflow and taken from the module train.\n",
    "\n",
    "We will compute the gradients by a function provided by the optimizer object called compute_gradients\n",
    "\n",
    "Clipped gradients mean the gradients are clipped to a particular value, below and above which our gradient values cannot go(to avoid vanishing and exploding gradient problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCqMyrMTcP9Q"
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"optimization\"):\n",
    "    loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,\n",
    "                                                  targets,\n",
    "                                                  tf.ones([input_shape[0], sequence_length]))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = optimizer.compute_gradients(loss_error)\n",
    "    clipped_gradients = [(tf.clip_by_value(grad_tensor, -5., 5.), grad_variable) for grad_tensor, grad_variable in gradients if grad_tensor is not None]\n",
    "    optimizer_gradient_clipping = optimizer.apply_gradients(clipped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0gbRgCRcP9Q"
   },
   "source": [
    "# Padding the sequences with the < PAD> token\n",
    "\n",
    "Why do we need to do padding ?\n",
    "\n",
    "All the sentences in a batch, whether they are questions or answers must have the same length. This a must do in Deep NLP.\n",
    "\n",
    "Creating a function called apply_padding.\n",
    "\n",
    "It will take 2 arguments:\n",
    "\n",
    "1.batch_of_sequences and 2.word2int dictionary\n",
    "\n",
    "The task of the function is to complete the sentences using PAD tokens so that all the sentences in the batch have the same length.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SecqjHTcP9R"
   },
   "outputs": [],
   "source": [
    "def apply_padding(batch_of_sequences, word2int):\n",
    "    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
    "    return [sequence + [word2int['<PAD>']] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZrtGk4DcP9R"
   },
   "source": [
    "--------------------------------------------------------------------------------\n",
    "# Splitting the data into batches of questions and answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "630qeH0KcP9S"
   },
   "source": [
    "We can naturally guess the arguments of this function:\n",
    "\n",
    "1.questions\n",
    "\n",
    "2.answers\n",
    "\n",
    "3.batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9KC0OBmccP9S"
   },
   "outputs": [],
   "source": [
    "def split_into_batches(questions, answers, batch_size):\n",
    "    for batch_index in range(0, len(questions) // batch_size):\n",
    "        start_index = batch_index * batch_size\n",
    "        questions_in_batch = questions[start_index : start_index + batch_size]\n",
    "        answers_in_batch = answers[start_index : start_index + batch_size]\n",
    "        padded_questions_in_batch = np.array(apply_padding(questions_in_batch, questionswords2int))\n",
    "        padded_answers_in_batch = np.array(apply_padding(answers_in_batch, answerswords2int))\n",
    "        yield padded_questions_in_batch, padded_answers_in_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZE65pHycP9T"
   },
   "source": [
    "--------------------------------------------------------------------------------\n",
    "# Splitting the questions and answers into training and validation sets\n",
    "\n",
    "The valiation set is keeping 10%-15% of the training data as validation set and will not be used for training the neural network. It will be used to check the predictive power of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3nd_3LzvcP9T"
   },
   "outputs": [],
   "source": [
    "training_validation_split = int(len(sorted_clean_questions) * 0.15)\n",
    "training_questions = sorted_clean_questions[training_validation_split:]\n",
    "training_answers = sorted_clean_answers[training_validation_split:]\n",
    "validation_questions = sorted_clean_questions[:training_validation_split]\n",
    "validation_answers = sorted_clean_answers[:training_validation_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U38suD_cP9U"
   },
   "source": [
    "--------------------------------------------------------------------------------\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Is005s-wcP9V"
   },
   "outputs": [],
   "source": [
    "batch_index_check_training_loss = 100\n",
    "batch_index_check_validation_loss = ((len(training_questions)) // batch_size // 2) - 1\n",
    "total_training_loss_error = 0\n",
    "list_validation_loss_error = []\n",
    "early_stopping_check = 0\n",
    "early_stopping_stop = 1000\n",
    "checkpoint = \"chatbot_weights.ckpt\" # For Windows users, replace this line of code by: checkpoint = \"./chatbot_weights.ckpt\"\n",
    "session.run(tf.global_variables_initializer())\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_questions, training_answers, batch_size)):\n",
    "        starting_time = time.time()\n",
    "        _, batch_training_loss_error = session.run([optimizer_gradient_clipping, loss_error], {inputs: padded_questions_in_batch,\n",
    "                                                                                               targets: padded_answers_in_batch,\n",
    "                                                                                               lr: learning_rate,\n",
    "                                                                                               sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                                               keep_prob: keep_probability})\n",
    "        total_training_loss_error += batch_training_loss_error\n",
    "        ending_time = time.time()\n",
    "        batch_time = ending_time - starting_time\n",
    "        if batch_index % batch_index_check_training_loss == 0:\n",
    "            print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epoch,\n",
    "                                                                                                                                       epochs,\n",
    "                                                                                                                                       batch_index,\n",
    "                                                                                                                                       len(training_questions) // batch_size,\n",
    "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n",
    "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n",
    "            total_training_loss_error = 0\n",
    "        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n",
    "            total_validation_loss_error = 0\n",
    "            starting_time = time.time()\n",
    "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_questions, validation_answers, batch_size)):\n",
    "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n",
    "                                                                       targets: padded_answers_in_batch,\n",
    "                                                                       lr: learning_rate,\n",
    "                                                                       sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                       keep_prob: 1})\n",
    "                total_validation_loss_error += batch_validation_loss_error\n",
    "            ending_time = time.time()\n",
    "            batch_time = ending_time - starting_time\n",
    "            average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\n",
    "            print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "            list_validation_loss_error.append(average_validation_loss_error)\n",
    "            if average_validation_loss_error <= min(list_validation_loss_error):\n",
    "                print('I speak better now!!')\n",
    "                early_stopping_check = 0\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session, checkpoint)\n",
    "            else:\n",
    "                print(\"Sorry I do not speak better, I need to practice more.\")\n",
    "                early_stopping_check += 1\n",
    "                if early_stopping_check == early_stopping_stop:\n",
    "                    break\n",
    "    if early_stopping_check == early_stopping_stop:\n",
    "        print(\"My apologies, I cannot speak better anymore. This is the best I can do.\")\n",
    "        break\n",
    "print(\"Game Over\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjRwjnPkcP9V"
   },
   "source": [
    "--------------------------------------------------------------------------------\n",
    "\n",
    "# PART 4 - TESTING THE SEQ2SEQ MODEL\n",
    "\n",
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayijKBdlcP9W"
   },
   "source": [
    "# Loading the weights and running the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n83-c61xcP9X"
   },
   "outputs": [],
   "source": [
    "checkpoint = \"./chatbot_weights.ckpt\"\n",
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(session, checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrSXEQ-ucP9Y"
   },
   "source": [
    "--------------------------------------------------------------------------------\n",
    "# Converting the questions from strings to list of encoding integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qLR5oiDcP9Z"
   },
   "outputs": [],
   "source": [
    "def convert_string2int(question, word2int):\n",
    "    question = clean_text(question)\n",
    "    return [word2int.get(word, word2int['<OUT>']) for word in question.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgjHdblscP9a"
   },
   "source": [
    "--------------------------------------------------------------------------------\n",
    "# Setting up the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hiv_xOhKcP9b"
   },
   "outputs": [],
   "source": [
    "while(True):\n",
    "    question = input(\"You: \")\n",
    "    if question == 'Goodbye':\n",
    "        break\n",
    "    question = convert_string2int(question, questionswords2int)\n",
    "    question = question + [questionswords2int['<PAD>']] * (25 - len(question))\n",
    "    fake_batch = np.zeros((batch_size, 25))\n",
    "    fake_batch[0] = question\n",
    "    predicted_answer = session.run(test_predictions, {inputs: fake_batch, keep_prob: 0.5})[0]\n",
    "    answer = ''\n",
    "    for i in np.argmax(predicted_answer, 1):\n",
    "        if answersints2word[i] == 'i':\n",
    "            token = ' I'\n",
    "        elif answersints2word[i] == '<EOS>':\n",
    "            token = '.'\n",
    "        elif answersints2word[i] == '<OUT>':\n",
    "            token = 'out'\n",
    "        else:\n",
    "            token = ' ' + answersints2word[i]\n",
    "        answer += token\n",
    "        if token == '.':\n",
    "            break\n",
    "    print('ChatBot: ' + answer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Deep NLP Chat Bot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
